{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2105ff9",
      "metadata": {
        "id": "c2105ff9"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a79f300",
      "metadata": {
        "id": "6a79f300"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab98d04",
      "metadata": {},
      "source": [
        "#### Initialize enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "558aa55f",
      "metadata": {
        "id": "558aa55f"
      },
      "outputs": [],
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5221d20",
      "metadata": {
        "id": "a5221d20"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3426ca21",
      "metadata": {
        "id": "3426ca21"
      },
      "outputs": [],
      "source": [
        "inputs = env.observation_space.shape # shape is (4,) in this case\n",
        "actions_num = env.action_space.n\n",
        "hidden_neurons = 256 # Arbitary choice\n",
        "\n",
        "in_layer = layers.Input(shape=inputs)\n",
        "hidden_layer = layers.Dense(hidden_neurons, activation=\"relu\")(in_layer)\n",
        "act_layer = layers.Dense(actions_num, activation=\"softmax\")(hidden_layer)\n",
        "val_layer = layers.Dense(1)(hidden_layer)\n",
        "\n",
        "model = keras.Model(inputs=in_layer, outputs=[act_layer, val_layer])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1742ec82",
      "metadata": {
        "id": "1742ec82"
      },
      "source": [
        "### Parameter Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba367400",
      "metadata": {
        "id": "ba367400"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99 # Discount for future rewards\n",
        "max_eps = 10**4\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "ep_rewards = []\n",
        "\n",
        "\n",
        "# Experience\n",
        "reward_history = []\n",
        "value_history = []\n",
        "action_probs_history = []\n",
        "\n",
        "# Arguments for display\n",
        "count = 0\n",
        "render = False # Option to render every step\n",
        "consecutive_max_r = 8 # Number of times the agent needs to hit the maximum reward to be considered fully trained \n",
        "progress_eps = 10 #Progress will be shown every progress_eps episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80bd1e1",
      "metadata": {
        "id": "d80bd1e1"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99fe7985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99fe7985",
        "outputId": "d55382a7-3276-4faa-b436-11ba8ec9a033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0,   Episode_Reward: 16.0\n",
            "Average reward of last 10 episodes: 0.0\n",
            "\n",
            "Episode: 10,   Episode_Reward: 12.0\n",
            "Average reward of last 10 episodes: 21.3\n",
            "\n",
            "Episode: 20,   Episode_Reward: 12.0\n",
            "Average reward of last 10 episodes: 22.5\n",
            "\n",
            "Episode: 30,   Episode_Reward: 42.0\n",
            "Average reward of last 10 episodes: 28.6\n",
            "\n",
            "Episode: 40,   Episode_Reward: 146.0\n",
            "Average reward of last 10 episodes: 103.1\n",
            "\n",
            "Reached Maximum reward for the 1th consecutive time\n",
            "Episode: 50,   Episode_Reward: 500.0\n",
            "Average reward of last 10 episodes: 183.1\n",
            "\n",
            "Reached Maximum reward for the 2th consecutive time\n",
            "Episode: 60,   Episode_Reward: 177.0\n",
            "Average reward of last 10 episodes: 261.3\n",
            "\n",
            "Episode: 70,   Episode_Reward: 124.0\n",
            "Average reward of last 10 episodes: 111.8\n",
            "\n",
            "Reached Maximum reward for the 1th consecutive time\n",
            "Episode: 80,   Episode_Reward: 461.0\n",
            "Average reward of last 10 episodes: 173.4\n",
            "\n",
            "Episode: 90,   Episode_Reward: 120.0\n",
            "Average reward of last 10 episodes: 113.4\n",
            "\n",
            "Reached Maximum reward for the 1th consecutive time\n",
            "Reached Maximum reward for the 2th consecutive time\n",
            "Reached Maximum reward for the 3th consecutive time\n",
            "Reached Maximum reward for the 4th consecutive time\n",
            "Episode: 100,   Episode_Reward: 480.0\n",
            "Average reward of last 10 episodes: 353.6\n",
            "\n",
            "Episode: 110,   Episode_Reward: 206.0\n",
            "Average reward of last 10 episodes: 308.6\n",
            "\n",
            "Episode: 120,   Episode_Reward: 30.0\n",
            "Average reward of last 10 episodes: 123.7\n",
            "\n",
            "Episode: 130,   Episode_Reward: 133.0\n",
            "Average reward of last 10 episodes: 42.1\n",
            "\n",
            "Episode: 140,   Episode_Reward: 148.0\n",
            "Average reward of last 10 episodes: 111.2\n",
            "\n",
            "Episode: 150,   Episode_Reward: 152.0\n",
            "Average reward of last 10 episodes: 136.6\n",
            "\n",
            "Episode: 160,   Episode_Reward: 188.0\n",
            "Average reward of last 10 episodes: 158.1\n",
            "\n",
            "Episode: 170,   Episode_Reward: 309.0\n",
            "Average reward of last 10 episodes: 200.3\n",
            "\n",
            "Reached Maximum reward for the 1th consecutive time\n",
            "Reached Maximum reward for the 1th consecutive time\n",
            "Reached Maximum reward for the 2th consecutive time\n",
            "Reached Maximum reward for the 3th consecutive time\n",
            "Reached Maximum reward for the 4th consecutive time\n",
            "Episode: 180,   Episode_Reward: 500.0\n",
            "Average reward of last 10 episodes: 453.7\n",
            "\n",
            "Reached Maximum reward for the 5th consecutive time\n",
            "Reached Maximum reward for the 6th consecutive time\n",
            "Reached Maximum reward for the 7th consecutive time\n",
            "Reached Maximum reward for the 8th consecutive time\n",
            "AGENT TRAINED!\n",
            "HIT 8 CONSECUTIVE MAXIMUM REWARDS\n"
          ]
        }
      ],
      "source": [
        "for ep in range(max_eps+1):\n",
        "    state = env.reset()\n",
        "    ep_reward = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        while True:\n",
        "            # Get the NN output of current state\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "            action_probs, value = model(state)\n",
        "            \n",
        "            # Squeez Tensor to 1-d array\n",
        "            np_probs = np.squeeze(action_probs)\n",
        "            # Get random action based on probability array\n",
        "            action = np.random.choice(actions_num, p=np_probs)\n",
        "            # Take step, get new state\n",
        "            state, reward, done, info = env.step(action)\n",
        "            \n",
        "            # Render enviroment\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            # Record experience\n",
        "            reward_history.append(reward)\n",
        "            value_history.append(value[0,0])\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Episode accumulative reward\n",
        "            ep_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Save each episode rewards to see progress\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        # For each reward r calculate corresponding expected value based on experience gathered\n",
        "        returns = [] # Expected value matrix for each action\n",
        "        expected_reward = 0\n",
        "        for r in reward_history:\n",
        "            expected_reward = r + gamma * expected_reward\n",
        "            returns.insert(0, expected_reward)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns))\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculate the loss\n",
        "        history = zip(action_probs_history, value_history, returns)\n",
        "        action_losses = []\n",
        "        value_losses = []\n",
        "        for log_prob, value, E_value in history:\n",
        "            Advantage = value - E_value # The return we got minus the return we expected\n",
        "            action_losses.append(log_prob * Advantage)  # Policy gradient loss\n",
        "\n",
        "            value_losses.append( # Huber loss\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(E_value, 0))\n",
        "            )\n",
        "        total_loss = sum(value_losses) + sum(action_losses)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        \n",
        "        # Reset histories\n",
        "        reward_history.clear()\n",
        "        value_history.clear()\n",
        "        action_probs_history.clear()\n",
        "\n",
        "    # Show progress every 10 episodes\n",
        "    if ep % 10 == 0:\n",
        "        print(f\"Episode: {ep},   Episode_Reward: {ep_reward}\")\n",
        "        print(f\"Average reward of last 10 episodes: {sum(ep_rewards[ep-10:ep])/10}\\n\")\n",
        "    # If we hit the maximum possible reward\n",
        "    if ep_reward == 500:\n",
        "      count += 1\n",
        "      print(f\"Reached Maximum reward for the {count}th consecutive time\")\n",
        "      # If we hit it 8 consecutive times consider agent fully trained\n",
        "      if count == consecutive_max_r:\n",
        "        print(\"AGENT TRAINED!\\nHIT 8 CONSECUTIVE MAXIMUM REWARDS\")\n",
        "        env.close()\n",
        "        break\n",
        "    else:\n",
        "      count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7t2WxmRb4ex0",
      "metadata": {
        "id": "7t2WxmRb4ex0"
      },
      "source": [
        "### Test agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "87dd63c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "87dd63c9",
        "outputId": "054e37bd-719f-4e7f-f552-ce0e495a1a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward achieved: 500.0\n"
          ]
        }
      ],
      "source": [
        "def AgentTest():\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    state = tf.expand_dims(state,0)\n",
        "    action, value = model(state)\n",
        "    np_probs = np.squeeze(action)\n",
        "    action = np.random.choice(actions_num, p=np_probs)      \n",
        "    state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "    if done:\n",
        "      break\n",
        "  print(f\"Total reward achieved: {total_reward}\")\n",
        "  env.close()\n",
        "\n",
        "AgentTest()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CartPole-v1_ActorCritic_Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
