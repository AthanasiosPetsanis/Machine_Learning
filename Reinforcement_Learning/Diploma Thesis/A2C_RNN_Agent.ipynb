{"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"x15VMA-8mIoW"}},{"cell_type":"markdown","metadata":{"id":"WaTXoEkNvm93"},"source":["### Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"BDMVfRMOvm97"},"outputs":[],"source":["import gym\n","import textworld.gym\n","from textworld.generator import make_game, compile_game\n","from textworld import EnvInfos\n","\n","import tensorflow as tf\n","import keras\n","from keras import layers, Model, Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time"]},{"cell_type":"markdown","metadata":{"id":"X55EsVV5vm99"},"source":["### Load Game\n","Get the first state of the game"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"hfQcisI7vm9-"},"outputs":[],"source":["# Register a text-based game as a new Gym's environment.\n","\n","# --- Make random game ---\n","\n","# options = textworld.GameOptions()\n","# options.path = \"./Random_Games/\"\n","# options.seeds = 432\n","# game = make_game(options)\n","# game_file = compile_game(game, options)\n","# print(game_file)\n","# -----------------------------\n","\n","\n","# --- Make random game with quick command ---\n","\n","# THIS DOES NOT OVERWRITE GAMES WITH EXISTING NAMES!\n","# !tw-make custom --world-size 2 --nb-objects 3 --theme house --quest-length 3 --entity-numbering --output \"./Created_Games/Sample_Game.ulx\" --seed 3 --format ulx\n","\n","# -------------------------------------\n","\n","# Load Game\n","def load_game(game_file, max_steps=100):\n","\n","    request_infos = EnvInfos(inventory=True, admissible_commands=True, entities=True, won=True, lost=True, last_command=True,\n","                             description=True, location=True, objective=True, policy_commands= True, score=True, moves=True)\n","\n","    env_id = textworld.gym.register_games([game_file], request_infos, max_episode_steps=max_steps)\n","\n","    env = gym.make(env_id)  # Start the environment.\n","    env.max_episode_steps = max_steps\n","    obs, infos = env.reset()# Start new episode.\n","\n","    print(obs)\n","\n","    score, moves, done = 0, 0, False\n","    return env, obs, infos"]},{"cell_type":"code","source":["# Easy Difficulty\n","\n","# Load env\n","game_file = \"./Created_Games/MDP_Game_Easy.ulx\"\n","env, obs, infos = load_game(game_file=game_file)"],"metadata":{"id":"ixXF8thVygWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9HIkdcavm-A"},"outputs":[],"source":["# Infos before processing\n","obs, score, done, infos = env.step(\"go east\")\n","for key, value in infos.items():\n","    print(key, ' : ', value, '\\n')"]},{"cell_type":"markdown","metadata":{"id":"_xmq2Vp-vm-A"},"source":["### Needed Info processing\n","Functions to get the **location** and remove **'look'**,**'invetory'** from the admissible_commands.<br/>\n","Also the **inventory** doesn't need the string **\"You are carrying:\"** as our input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oFKBeiPvm-B"},"outputs":[],"source":["# infos[\"location\"] always returns None for some reason\n","# So we need to get it manually from description\n","\n","ban_list = ['You', 'are', 'carrying', 'nothing.', '\\n']\n","\n","def find_location(desc):\n","    desc = desc.split()\n","    location = desc[1]\n","    return location\n","    \n","# Also we don't need the \"inventory\" and \"look\" commands\n","# nor do we need \"examine\" commands cause we don't use infos[\"description\"] feedback\n","def cmd_remover(ad_cmds):\n","    ad_cmds.remove(\"inventory\")\n","    ad_cmds.remove(\"look\")\n","    ad_cmds = [cmd for cmd in ad_cmds if 'examine' not in cmd]\n","    return ad_cmds\n","\n","def inv_process(inventory):\n","    \n","    try:\n","        inventory = inventory.split()\n","        inventory = [item for item in inventory if all(word not in item for word in ban_list)]\n","        if 'a' in inventory: inventory.remove('a')\n","        if 'an' in inventory: inventory.remove('an')\n","        return inventory \n","    except:\n","        print(\"Inventory cannot be split\")\n","        return infos[\"inventory\"]\n","\n","def obj_process(commands):\n","    try:\n","        objective = commands[-1]\n","    except:\n","        objective = infos['last_command']\n","\n","    return objective\n","    \n","def info_process():\n","    \n","    # Pre-process inventory\n","    infos[\"inventory\"] = inv_process(infos[\"inventory\"])\n","        \n","    # Find location\n","    infos[\"location\"] = find_location(infos[\"description\"])\n","\n","    # Get goal\n","    infos[\"objective\"] = obj_process(infos[\"policy_commands\"])\n","    \n","    # Get only needed commands\n","    try:\n","        commands = cmd_remover(infos[\"admissible_commands\"])\n","        infos[\"admissible_commands\"] = commands\n","    except:\n","        print(\"'Look','invetory' and 'examine' commands already cut\\n\")\n","        commands = infos[\"admissible_commands\"]\n","        \n","    # Return processed observation\n","    obs = [infos[\"location\"], infos[\"inventory\"], infos[\"objective\"], infos[\"entities\"]]\n","    for cmd in commands:\n","        obs.append(cmd)\n","        \n","    return obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw4J2VVHvm-C"},"outputs":[],"source":["obs = info_process()\n","for i in obs[0:4]:\n","    print(i,'\\n')\n","print(obs[4::])"]},{"cell_type":"markdown","metadata":{"id":"LNTQM8PFvm-C"},"source":["### Extract first model inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"BZEK_kKUvm-D"},"outputs":[],"source":["vocab_size = 200\n","tokenizer = Tokenizer(num_words = vocab_size)\n","\n","def tokenize():\n","    obs = info_process()\n","    # print(obs)\n","    tokenizer.fit_on_texts(obs)\n","    input_info = tokenizer.texts_to_sequences(obs)\n","    location, inventory, objective, entities = input_info[0:4] # These don't need padding\n","    commands = pad_sequences(input_info[4::], padding='post') # Pad the commands\n","    return location, inventory, objective, entities, commands\n","    \n","location, inventory, objective, entities, commands = tokenize()\n","print(tokenizer.word_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"4vs_CoyZvm-E"},"outputs":[],"source":["# Show the padding being done\n","\n","print(commands)\n","obs, score, done, infos = env.step(\"take latchkey\")\n","print(infos[\"inventory\"])\n","location, inventory, objective, entities, commands = tokenize()\n","print(commands)\n","print(infos[\"admissible_commands\"])\n","print(inventory)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"NwHujLvRvm-E"},"outputs":[],"source":["\n","def input_process(location, inventory, objective, entities, commands):\n","    pre_words = location + inventory + entities\n","    words = []\n","    pre_objective = objective\n","    obj = []\n","    \n","    for i in range(np.shape(commands)[0]):\n","        words.append(pre_words)\n","        obj.append(pre_objective)\n","        \n","    words = tf.convert_to_tensor(words)\n","    obj = tf.convert_to_tensor(obj)\n","    commands = tf.convert_to_tensor(commands)\n","    \n","    return words, obj, commands\n","words, objective, commands = input_process(location, inventory, objective, entities, commands)\n","objective"]},{"cell_type":"markdown","metadata":{"id":"pXyB_7eHvm-E"},"source":["### Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"qQwEQAxZvm-F"},"outputs":[],"source":["# Number of neurons for the layers\n","embedding_dim = 50\n","arb_vec = round(embedding_dim/2)\n","\n","# Same embedding layer for everything\n","emb_layer = layers.Embedding(vocab_size, embedding_dim)\n","\n","# The word info (inventory, entities, location) will go into a GRU\n","word_info = layers.Input(shape=(1,), name=\"Words(Location,Inventory,Entitites)\")\n","emb_words = emb_layer(word_info)\n","enc_words = layers.GRU(arb_vec)(emb_words)\n","WRDS = Model(inputs=word_info, outputs=enc_words)\n","\n","# The objective will go into a LSTM encoder(Long Short-Term Memory)\n","# because we care about remembering all the sentense (all the described goals)\n","obj_info = layers.Input(shape=(1,), name='Objective')\n","emb_obj = emb_layer(obj_info)\n","enc_obj = layers.GRU(arb_vec)(emb_obj)\n","OBJ = Model(inputs=obj_info, outputs=enc_obj)\n","\n","# The commands will be encoded with a different LSTM\n","cmds_info = layers.Input(shape=(1,), name='Admissible_Commands')\n","emb_cmds = emb_layer(cmds_info)\n","enc_cmds = layers.GRU(arb_vec)(emb_cmds)\n","CMDS = Model(inputs=cmds_info, outputs=enc_cmds)\n","\n","# The layers are combined so that value and index are calculated based on all the observations\n","combined = layers.Concatenate(axis=1)([WRDS.output, OBJ.output, CMDS.output])\n","pre_index = layers.Dense(10, activation='softmax')(combined)\n","# Find value,index from a 1-neuron layer\n","index = layers.Dense(1, activation=\"sigmoid\", name=\"Probability_of_action\")(pre_index)\n","value = layers.Dense(1, name='Value_of_action')(pre_index)\n","\n","\n","# The complete model\n","agent = Model(inputs=[WRDS.input, OBJ.input, CMDS.input], outputs=[index, value])\n","opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n","huber_loss=keras.losses.Huber()\n","agent.compile(optimizer=opt, loss=huber_loss,\n","              metrics=[tf.keras.metrics.BinaryAccuracy(),\n","                       tf.keras.metrics.FalseNegatives()])\n","agent.summary()\n","agent.save(\"agent.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VY_cELGIvm-F"},"outputs":[],"source":["# Example Input after tokenization\n","# We assume batch size to be equal to the number of commands\n","# in other words produce num_of_commands indexes and values\n","\n","# Since the rest of the inputs will be the same for every command in the current step\n","# we need to replicate it that many times\n","# Assuming num_of_commands = 3\n","\n","words = np.arange(20)\n","words = [words, words, words] \n","\n","objective = np.arange(20,30)\n","objective = [objective, objective, objective]\n","\n","commands = np.zeros([3,4])\n","commands[1][:] = 1\n","commands[2][:] = 2\n","\n","# This way we have the same batch_size for every input, but the only thing changing is the command\n","\n","words = tf.convert_to_tensor(words)\n","objective = tf.convert_to_tensor(objective)\n","commands = tf.convert_to_tensor(commands)\n","\n","print(\"Example of inputs\")\n","print(\"Words: \\n{}\\nObjective:\\n {}\\nCommands:\\n {}\\n\".format(words, objective, commands))\n","[index, value] = agent([words, objective, commands])\n","print(\"Index: \\n{}\\nValue: \\n{}\\n\".format(index,value))"]},{"cell_type":"markdown","source":["### Parameters"],"metadata":{"id":"SVnn08pdIVBb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMegWmwSvm-G"},"outputs":[],"source":["epsilon = np.finfo(np.float32).eps.item() \n","\n","# Experience\n","ep_rewards = []\n","reward_history = []\n","value_history = []\n","action_probs_history = []\n","percentage_won_overtime = []\n","moves_history = []\n","avg_moves = []\n","won_games = 0\n","\n","cnt = 0 # Step count"]},{"cell_type":"markdown","metadata":{"id":"wMrLFITlvm-G"},"source":["### Training through batches of experience"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"BsbWNlgRvm-G"},"outputs":[],"source":[" def train(max_eps=100, reset=False, render=False, gamma=0.99, lrn_rate=0.01, metrics_bs=10, neg_reward=False):\n"," \n","    global ep_rewards, reward_history, value_history, action_probs_history, percentage_won_overtime, moves_history,\\\n","             avg_moves, won_games, cnt, infos\n","\n","    if reset:\n","        ep_rewards = []\n","        reward_history = []\n","        value_history = []\n","        action_probs_history = []\n","        percentage_won_overtime = []\n","        moves_history = []\n","        avg_moves = []\n","        won_games = 0\n","\n","    start = time.time()\n","\n","    for ep in range(max_eps):\n","        obs, infos = env.reset()\n","        ep_reward = 0\n","        \n","        with tf.GradientTape() as tape:\n","            while True:\n","                \n","                # print(cnt)\n","                cnt += 1 # for debugging\n","                \n","                # Every Step tokenize and prepare the inputs\n","                # Then let model take decision\n","                location, inventory, objective, entities, commands = tokenize()\n","                words, objective, commands = input_process(location, inventory, objective, entities, commands)\n","                action_probs, value = agent([words, objective, commands])\n","                cmd_len = len(commands) # Basically the batch_size\n","                \n","                # Choose command based on probability\n","                np_probs = np.squeeze(action_probs)\n","                np_probs = np_probs/np.sum(np_probs) if np.sum(np_probs) > 0 else [1/cmd_len for i in range(cmd_len)] #In case sum is 0\n","                if np.shape(commands)[0] == 1: # If we have 1 possible action np_probs becomes a float\n","                    np_probs = [np_probs]      # We need it to be a list\n","                    action_index = 0\n","                else:\n","                    action_index = np.random.choice(cmd_len, p=np_probs)\n","                action = infos[\"admissible_commands\"][action_index]\n","                \n","                # Take step\n","                obs, reward, done, infos = env.step(action)\n","\n","                if neg_reward: reward -= 0.5\n","                \n","                # Render enviroment\n","                if render:\n","                    env.render()\n","                    \n","                # Record experience\n","                reward_history.append(reward)\n","                value_history.append(value[action_index])\n","                action_probs_history.append(tf.math.log(action_probs[action_index]))\n","                \n","                # Episode accumulative reward\n","                ep_reward += reward\n","                \n","                if done:\n","                    last_r_index = len(reward_history)-1\n","                    moves_history.append(infos['moves'])\n","                    if infos[\"won\"]:\n","                        ep_reward += 10\n","                        reward_history[last_r_index] += 10\n","                        won_games += 1\n","                    else:\n","                        ep_reward -= 10\n","                        reward_history[last_r_index] -= 10\n","                        print(\"We lost in {} moves\".format(infos[\"moves\"]))\n","                    break\n","                \n","            # Save each episode rewards to see progress\n","            ep_rewards.append(ep_reward)\n","            \n","            # For each reward r calculate corresponding expected value based on experience gathered\n","            returns = [] # Expected value matrix for each action\n","            expected_reward = 0\n","            for r in reward_history[::-1]:\n","                expected_reward = r + gamma * expected_reward\n","                returns.insert(0, expected_reward)\n","                \n","            # Normalize\n","            returns = np.array(returns)\n","            returns = (returns - np.mean(returns)) / (np.std(returns) + epsilon)\n","            returns = returns.tolist()\n","            \n","            # Calculate the loss\n","            history = zip(action_probs_history, value_history, returns)\n","            action_losses = []\n","            value_losses = []\n","            for log_prob, value, E_value in history:\n","                Advantage = value - E_value # The return we got minus the return we expected\n","                action_losses.append(log_prob * Advantage)  # Policy gradient loss\n","\n","                value_losses.append( # Huber loss\n","                    huber_loss(tf.expand_dims(value, 0), tf.expand_dims(E_value, 0))\n","                )\n","            total_loss = sum(value_losses) + sum(action_losses)\n","            \n","            # Backpropagation\n","            grads = tape.gradient(total_loss, agent.trainable_variables)\n","            opt.apply_gradients(zip(grads, agent.trainable_variables))\n","            \n","            # Reset histories\n","            reward_history.clear()\n","            value_history.clear()\n","            action_probs_history.clear()\n","\n","        # Metrics\n","        if (ep+1)%metrics_bs==0:\n","            percentage_won_overtime.append(won_games/metrics_bs)\n","            won_games = 0\n","            avg_moves.append(sum(moves_history[-metrics_bs::])/metrics_bs)\n","            print(\"Currently on Episode {}\".format(ep+1))\n","        \n","    plt.figure(1)\n","    plt.plot(np.arange(1,len(avg_moves)+1), avg_moves)\n","    plt.xlabel(f'Every {metrics_bs} episodes')\n","    plt.ylabel(f'Average moves every {metrics_bs} eps')\n","    plt.figure(2)\n","    plt.plot(np.arange(1,len(percentage_won_overtime)+1), percentage_won_overtime)\n","    plt.xlabel(f'Every {metrics_bs} episodes')\n","    plt.ylabel(f'Percentage of games won every {metrics_bs} eps')\n","    plt.show()\n","    end = time.time(); t_sec = end-start; mins = t_sec//60; secs = t_sec-mins*60\n","    print(f\"Training took {mins} minutes and {secs} seconds\")"]},{"cell_type":"code","source":["name1 = 'bedroom'\n","name2 = 'Bedroom'\n","if name1==name2.lower(): print(\"LUL\")"],"metadata":{"id":"XNVjwZ-PHoWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(render=False, max_eps=1):\n","    global infos\n","\n","    for ep in range(max_eps):\n","\n","        print(f\"\\n----------- EPISODE {ep+1} -----------\")\n","        actions_taken = []\n","        step = 0\n","        obs, infos = env.reset()\n","        while True:\n","            step += 1\n","            if render:\n","                env.render()\n","\n","            # Every Step tokenize and prepare the inputs\n","            # Then let model take decision\n","            location, inventory, objective, entities, commands = tokenize()\n","            words, objective, commands = input_process(location, inventory, objective, entities, commands)\n","            action_probs, value = agent([words, objective, commands])\n","            cmd_len = len(commands) # Basically the batch_size\n","            \n","            # Choose command based on probability\n","            np_probs = np.squeeze(action_probs)\n","            np_probs = np_probs/np.sum(np_probs) if np.sum(np_probs) > 0 else [1/cmd_len for i in range(cmd_len)] #In case sum is 0\n","\n","            if np.shape(commands)[0] == 1: # If we have 1 possible action np_probs becomes a float\n","                np_probs = [np_probs]      # We need it to be a list\n","                action_index = 0\n","            else:\n","                action_index = np.random.choice(cmd_len, p=np_probs)\n","            action = infos[\"admissible_commands\"][action_index]\n","            \n","            print(f\"Action taken in step {step}: {action}\")\n","            actions_taken.append(action)\n","            \n","            obs, reward, done, infos = env.step(action)\n","            \n","            if done:\n","                print(f\"Finished in {infos['moves']} moves\")\n","                print(\"Actions taken:\")\n","                for action in actions_taken:\n","                    print(f\"{action} >\", end=\" \")\n","                break"],"metadata":{"id":"mwvBEJ8-_YFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Easy Difficulty\n","\n","# Train\n","train(max_eps=500, lrn_rate=0.001, neg_reward=True)\n","# If we train again it will resume training\n","train(max_eps=500, lrn_rate=0.001, neg_reward=True)\n","\n","# Test\n","test(max_eps=2)"],"metadata":{"cellView":"code","id":"Ba2Ms43euluZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fqUzurL5ZfjI"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"A2C_RNN_Agent.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}