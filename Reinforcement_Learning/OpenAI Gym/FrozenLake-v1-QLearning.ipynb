{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b52250",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33665663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bae94",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849b1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = 1; min_expl = 0.01; max_expl = 1\n",
    "learning_rate = 0.8\n",
    "gamma = 0.95\n",
    "expl_decay_rate = 0.005\n",
    "max_t = 99; max_eps = 1000; max_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff218a5",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6ea045",
   "metadata": {},
   "outputs": [],
   "source": [
    "render = False; LOG = False\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "q = np.zeros([env.observation_space.n, env.action_space.n]) # Q-value matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e517a7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a1df2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Score over time --------------\n",
      "Score for this epoch: 17.9% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 31.2% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 36.7% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 44.2% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 38.9% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 48.7% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 43.4% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 52.0% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 44.0% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 45.8% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 44.0% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 50.8% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 48.5% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 42.8% success rate\n",
      "-------------- Score over time --------------\n",
      "Score for this epoch: 48.7% success rate\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    total_epoch_rewards = 0\n",
    "    \n",
    "    for episode in range(max_eps):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            t += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "            # Randomly choose whether to explore or exploit based on probability expl\n",
    "            if random.uniform(0,1) > expl:\n",
    "                action = q[obs].argmax() # Exploit\n",
    "            else:\n",
    "                action = env.action_space.sample() # Explore\n",
    "\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            # New Q value of state S and action A is equal to some of the old Q value\n",
    "            # and the rest of the next Q value times gamma plus reward\n",
    "            q[obs, action] = (1-learning_rate) * q[obs,action] + learning_rate * (reward + gamma*np.max(q[new_obs]))\n",
    "            obs = new_obs\n",
    "\n",
    "            if LOG: print(f\"Action: {action},   Observation: {obs},   Reward: {reward}\")\n",
    "            if done:\n",
    "                if LOG: \n",
    "                    print(f\"! ---------------- GAME OVER ---------------- !\")\n",
    "                    if reward == 1: print(\"! ---------------- YOU WON ---------------- !\")\n",
    "                break\n",
    "                \n",
    "        total_epoch_rewards += reward\n",
    "        expl = min_expl + (max_expl - min_expl)*np.exp(-expl_decay_rate*(episode+1)*(epoch+1)) \n",
    "#         expl -= expl_decay_rate; expl = max(min(expl, max_expl), min_expl)\n",
    "\n",
    "    rewards.append(total_epoch_rewards)\n",
    "    # Improvement every epoch\n",
    "    print (\"-------------- Score over time --------------\")\n",
    "    print (f\"Score for this epoch: {rewards[epoch]*100/max_eps}% success rate\")\n",
    "\n",
    "    # The percentage success rate is expected be worse during training due to exploration i.e. random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded5709b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ NEW MATRIX ------------ \n",
      " [[2.55262413e-01 1.02148042e-01 2.30091199e-01 9.02824539e-02]\n",
      " [1.36348608e-02 8.88763786e-03 2.55470230e-03 3.05152470e-01]\n",
      " [6.86141886e-03 4.97175663e-02 1.39986979e-02 2.62598746e-01]\n",
      " [7.18517817e-04 1.20253883e-03 7.97635689e-04 1.18684992e-01]\n",
      " [3.74958048e-01 1.16276348e-02 1.54936395e-03 2.13141095e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.24474492e-05 4.16912426e-06 2.17537696e-01 2.86595148e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.33985885e-02 6.30755948e-04 1.26496439e-02 3.49547613e-01]\n",
      " [2.53406695e-04 2.46218561e-01 7.63963578e-04 1.63145019e-02]\n",
      " [7.76579616e-01 2.65439681e-04 1.23402476e-03 2.22147848e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.30270596e-01 6.20280730e-01 8.57183468e-01 9.26525249e-02]\n",
      " [1.60044519e-01 9.84626561e-01 2.72438867e-01 2.28228929e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Resulted weights (Q-values)\n",
    "print(f\"------------ NEW MATRIX ------------ \\n {q}\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce11e98",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065a325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          -------------- Score --------------\n",
      "Percentage score: 72.6%\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "total_rewards = 0\n",
    "\n",
    "for episode in range(1000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = q[obs].argmax()\n",
    "\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        obs = new_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_rewards += reward\n",
    "\n",
    "# Improvement every epoch\n",
    "print (\"          -------------- Score --------------\")\n",
    "print (f\"Percentage score: {total_rewards/1000*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f153131",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9326c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 8,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 9,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 10,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Action: 0,   Observation: 9,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 8,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 4,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 8,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 4,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 0,   Reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 4,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 8,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 8,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 9,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 8,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 4,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Action: 0,   Observation: 8,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 9,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 10,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Action: 0,   Observation: 9,   Reward: 0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 8,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 8,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Action: 3,   Observation: 9,   Reward: 0.0\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Action: 1,   Observation: 13,   Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "Action: 2,   Observation: 14,   Reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Action: 1,   Observation: 15,   Reward: 1.0\n",
      "! ---------------- GAME   53   OVER ---------------- !\n",
      "! ---------------- YOU WON ---------------- !\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "t = 0\n",
    "\n",
    "while True:\n",
    "    t += 1\n",
    "    env.render()\n",
    "    action = q[obs].argmax() \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Action: {action},   Observation: {obs},   Reward: {reward}\")\n",
    "    time.sleep(1)\n",
    "    if done:\n",
    "        print(f\"! ---------------- GAME OVER ---------------- !\")\n",
    "        if reward == 1: print(\"! ----------------- YOU WON ----------------- !\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
